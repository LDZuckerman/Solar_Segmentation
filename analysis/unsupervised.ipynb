{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'funclib' from '/mnt/c/Users/Lea/OneDrive/SolarML_2023/Solar_Segmentation/funclib.py'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import torch.nn as nn\n",
    "import os, sys\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import astropy.io.fits as fits\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import importlib\n",
    "sys.path.insert(0, 'Solar_Segmentation/utils')\n",
    "from Solar_Segmentation.utils import data_utils, run_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See all models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a random sample of images\n",
    "\n",
    "WNet_name = 'WNet35nm'\n",
    "fig, ((ax1, ax2, ax3), (ax4, ax5, ax6), (ax7, ax8, ax9), (ax10, ax11, ax12), (ax13, ax14, ax15)) = plt.subplots(5, 3, figsize=(8, 13)); axs = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9, ax10, ax11, ax12, ax13, ax14, ax15]\n",
    "ax1.set_title('Image')\n",
    "ax2.set_title('Algorithmic Labels')\n",
    "ax3.set_title('WNet Labels')\n",
    "output_dir = f'../NN_outputs/{WNet_name}_outputs'\n",
    "target_pos = 2 if (WNet_name=='WNet15m' or WNet_name=='WNet16m') else 0\n",
    "for i in range(0, 14, 3):\n",
    "    idx = np.random.randint(0, len([file for file in os.listdir(output_dir) if file.startswith('x')]))\n",
    "    im = np.load(f'{output_dir}/x_{idx}.npy')[target_pos]\n",
    "    true = np.load(f'{output_dir}/true_{idx}.npy')\n",
    "    preds = np.squeeze(np.load(f'{output_dir}/pred_{idx}.npy'))\n",
    "    axs[i].imshow(im, cmap='gist_gray'); axs[i].set_ylabel(idx)\n",
    "    axs[i+1].imshow(true, vmin=0, vmax=1.5, cmap='gist_gray')\n",
    "    axs[i+2].imshow(preds, vmin=0, vmax=1.5, cmap='gist_gray')\n",
    "    axs[i].xaxis.set_tick_params(labelbottom=False); axs[i].yaxis.set_tick_params(labelleft=False); axs[i].set_xticks([]); axs[i].set_yticks([])\n",
    "    axs[i+1].xaxis.set_tick_params(labelbottom=False); axs[i+1].yaxis.set_tick_params(labelleft=False); axs[i+1].set_xticks([]); axs[i+1].set_yticks([])\n",
    "    axs[i+2].xaxis.set_tick_params(labelbottom=False); axs[i+2].yaxis.set_tick_params(labelleft=False); axs[i+2].set_xticks([]); axs[i+2].set_yticks([])\n",
    "plt.savefig(f'../NN_outputs/{WNet_name}_val_examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Compare models\n",
    "'''\n",
    "\n",
    "idx = np.random.randint(0, 108)\n",
    "print(idx) # 131 # 131 is good image for full DKIST seg v1 (Data/UNetData). HOWEVER note that \"true\" seg here is still version 1. Does not matter for training unsup, but perhaps for comparing validations.                                       #161 # np.random.randint(0, 250) \n",
    "#models = ['WNet1','WNet1m', 'WNet8', 'WNet12', 'WNet5', 'WNet6', 'WNet7', 'WNet2','WNet11','WNet10','WNet13', 'WNet14m','WNet15m', 'FreezeNet1', 'FreezeNet2', 'FreezeNet3','FreezeNet4']\n",
    "#models = ['WNet6', 'WNet2','WNet11', 'WNet14m','WNet15m', 'WNet20m','WNet22m', 'WNet25m', 'WNet27', 'WNet31nm', 'WNet35nm']\n",
    "#models = ['WNet1m', 'WNet2m', 'WNet8m'] # Binary models \n",
    "#models = ['WNet12','WNet5','WNet6'] # 3 class models \n",
    "#models = ['WNet15m','WNet16m'] # 3 class T series models \n",
    "#models = ['WNet19m','WNet22m','WNet25m','WNet26m'] # 3 class mag models\n",
    "models = list(model_dict.keys()) # models on properly normalized data!\n",
    "fig, axs = plt.subplots(len(models), 3, figsize=(9, 3*len(models)))\n",
    "for i in range(len(models)):\n",
    "    output_dir = f'../NN_outputs/{models[i]}_outputs'\n",
    "    target_pos = 2 if models[i]=='WNet15m' else 0\n",
    "    im = np.load(f'{output_dir}/x_{idx}.npy')[target_pos] # index to get image\n",
    "    true = np.load(f'{output_dir}/true_{idx}.npy')\n",
    "    preds = np.squeeze(np.load(f'{output_dir}/pred_{idx}.npy'))\n",
    "    if int(preds[40, 60]) == 0: # try to mostly have black be zero\n",
    "        preds_copy = np.copy(preds)\n",
    "        preds[preds_copy == 0.0] = 1\n",
    "        preds[preds_copy == 1.0] = 0 \n",
    "    axs[i,1].set_title(f'{models[i]}: {model_dict[models[i]]}')\n",
    "    axs[i,0].imshow(im, cmap='gist_gray')\n",
    "    axs[i,1].imshow(true, vmin=0, vmax=1.5, cmap='gist_gray')\n",
    "    axs[i,2].imshow(preds, vmin=0, vmax=1.5, interpolation='none', cmap='tab10') #cmap='gist_gray')#cmap='tab10')\n",
    "    axs[i,0].xaxis.set_tick_params(labelbottom=False); axs[i,0].yaxis.set_tick_params(labelleft=False); axs[i,0].set_xticks([]); axs[i,0].set_yticks([])\n",
    "    axs[i,1].xaxis.set_tick_params(labelbottom=False); axs[i,1].yaxis.set_tick_params(labelleft=False); axs[i,1].set_xticks([]); axs[i,1].set_yticks([])\n",
    "    axs[i,2].xaxis.set_tick_params(labelbottom=False); axs[i,2].yaxis.set_tick_params(labelleft=False); axs[i,2].set_xticks([]); axs[i,2].set_yticks([])\n",
    "plt.savefig('ExamplePredictions_OneImage.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traditional "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "path = \"../Data/All_DKIST/FirstLight.fits\"\n",
    "data = fits.open(path)[0].data\n",
    "labels = np.squeeze(fits.open(\"../Data/All_DKIST/SEGv2_FirstLight\")[0].data) # just to check against\n",
    "\n",
    "# Cut data to smaller size and flatten\n",
    "data = data[100:300, 100:300]\n",
    "labels = labels[100:300, 100:300]\n",
    "dataflat = data.reshape(-1)\n",
    "\n",
    "# Plot data for comparison\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(data, cmap='gray', origin='lower')\n",
    "plt.title('Initial data (HE)')\n",
    "plt.figure()\n",
    "\n",
    "# Create features and put into df (skip kernel feature for now - contain inf and probably not super useful anyway)\n",
    "df = pd.DataFrame()\n",
    "df['OG_value'] = dataflat\n",
    "df = funclib.add_gradient_feats(df, data) # Add value of (non-HE) gradient as feature\n",
    "df = funclib.add_sharpening_feats(df, dataflat) # Add value of sharpening filters as features RIGHT NOW JUST SQUARED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Two-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) KMeans (best) ############################################################################\n",
    "#\n",
    "#       Clusters by separating into n groups of equal variance, minimizing within-cluster sum-of-squares\n",
    "#       Overall, this seems to overestimate IGM\n",
    "#       3 clusters does not ID brightpoints (even with addition of gradient features) - but potentially is useful for IDing dim outskirts and centers\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "print('KMeans clustering')\n",
    "n_clusterss = [2]\n",
    "inits = ['k-means++', 'random'] # can also try passing locs of centers if use another alg to determine\n",
    "\n",
    "dict = {}\n",
    "i = 0\n",
    "for n_clusters in n_clusterss:\n",
    "    for init in inits:\n",
    "        preds_flat = KMeans(n_clusters=n_clusters, init=init, n_init=10).fit(df.values).labels_\n",
    "        preds = np.reshape(preds_flat, (np.shape(data)[0], np.shape(data)[1]))\n",
    "        preds = funclib.post_process(preds) # make sure IG is assigned to 0, G to 1\n",
    "        dict[str(i)] = [n_clusters, init, preds]\n",
    "        i += 1\n",
    "        pct_correct = len(np.where(preds.reshape(-1)==labels.reshape(-1))[0])/len(preds.reshape(-1))\n",
    "        print('    n_clusters='+str(n_clusters)+', init='+str(init)+ ', \"accuracy\":', pct_correct)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 3))\n",
    "fig.suptitle('KMeans')\n",
    "axs = [ax1, ax2]\n",
    "for i in range(len(axs)):\n",
    "    n_clusters =  dict[str(i)][0]\n",
    "    init = dict[str(i)][1]\n",
    "    preds = dict[str(i)][2]\n",
    "    axs[i].imshow(preds, origin='lower')\n",
    "    axs[i].set_title('n_clusters='+str(n_clusters)+', init='+str(init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2) AgglomorativeCluster ################################################################\n",
    "# #\n",
    "# #       Build clusters by finding closest pairs, merging iteratively \n",
    "# #       Does best with 3 clusters, 'complete' linkage, 'euclidean' metric:\n",
    "# #           Without gradient feature, 3rd cluster becomes rings around granules, not brightpoints\n",
    "# #           With gradient feature, maybe finds dim middles?\n",
    "\n",
    "# from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# print('AgglomerativeClustering')\n",
    "# n_clusterss = [2] \n",
    "# metrics = ['euclidean'] #  'l1', 'manhattan' seem to do similarly (at least with n_clusters=2) \n",
    "# linkages = ['complete', 'ward'] # 'average' and 'single' do terrible\n",
    "\n",
    "# dict = {}\n",
    "# i = 0\n",
    "# for n_clusters in n_clusterss:\n",
    "#     for metric in metrics:\n",
    "#         for linkage in linkages:\n",
    "#             preds_flat = AgglomerativeClustering(n_clusters=n_clusters, metric=metric, linkage=linkage).fit(df.values).labels_\n",
    "#             preds = np.reshape(preds_flat, (np.shape(data)[0], np.shape(data)[1]))\n",
    "#             preds = funclib.post_process(preds) # make sure IG is addigned to 0, G to 1\n",
    "#             dict[str(i)] = [n_clusters, metric, linkage, preds]\n",
    "#             i += 1\n",
    "#             pct_correct = len(np.where(preds.reshape(-1)==labels.reshape(-1))[0])/len(preds.reshape(-1))\n",
    "#             print('    n_clusters='+str(n_clusters)+', linkage='+str(linkage)+ ', \"accuracy\":', pct_correct)\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 3))\n",
    "# fig.suptitle('AgglomorativeClustering', color=tc)\n",
    "# axs = [ax1, ax2]\n",
    "# for i in range(len(axs)):\n",
    "#     n_clusters =  dict[str(i)][0]\n",
    "#     metric = dict[str(i)][1]\n",
    "#     linkage = dict[str(i)][2]\n",
    "#     preds = dict[str(i)][3]\n",
    "#     axs[i].imshow(preds, origin='lower')\n",
    "#     axs[i].set_title('n_clusters='+str(n_clusters)+', linkage='+str(linkage), color=tc)\n",
    "# for ax in axs: ax.tick_params(labelcolor=tc, color=tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) KMeans ############################################################################\n",
    "#   \n",
    "#   Use large n_clusters, but then combine some.\n",
    "#   Or: use only for outlineing? Could make into \"initial seg\" in algorithm?\n",
    "#       - seems like this would work best with at least 7 clusters\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Explore hyperparameters\n",
    "n_clusterss = [2, 3, 5, 7]\n",
    "inits = ['random']\n",
    "\n",
    "dict = {}\n",
    "i = 0\n",
    "for n_clusters in n_clusterss:\n",
    "    for init in inits:\n",
    "        preds_flat = KMeans(n_clusters=n_clusters, init=init, n_init=10).fit(df.values).labels_\n",
    "        preds = np.reshape(preds_flat, (np.shape(data)[0], np.shape(data)[1]))\n",
    "        dict[str(i)] = [n_clusters, init, preds]\n",
    "        i += 1\n",
    "fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5, figsize=(17, 3.5))\n",
    "#fig.suptitle('KMeans')\n",
    "axs = [ax1, ax2, ax3, ax4, ax5]\n",
    "axs[0].imshow(data, cmap='gist_gray', origin='lower'); axs[0].set_title('Image')\n",
    "axs[1].imshow(labels, cmap='gist_gray',  origin='lower'); axs[1].set_title('Algorithmic Segmentation')\n",
    "for i in range(len(axs)-2):\n",
    "    n_clusters =  dict[str(i)][0]\n",
    "    init = dict[str(i)][1]\n",
    "    preds = dict[str(i)][2]\n",
    "    axs[i+2].imshow(preds, origin='lower', cmap='gist_gray')\n",
    "    axs[i+2].set_title(f'KMeans with {n_clusters} clusters')#+', init='+str(init))\n",
    "for ax in axs:\n",
    "    ax.xaxis.set_tick_params(labelbottom=False); ax.yaxis.set_tick_params(labelleft=False)\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "# Using best hyperparameters\n",
    "n_clusters = 7\n",
    "preds_flat = KMeans(n_clusters=n_clusters, init='random', n_init=10).fit(df.values).labels_\n",
    "preds = np.reshape(preds_flat, (np.shape(data)[0], np.shape(data)[1]))\n",
    "\n",
    "def kmeans_to_seg(preds, data, resolution=0.016, bp_max_size=0.15):\n",
    "    seg = np.zeros_like(data)*np.NaN\n",
    "    # HE\n",
    "    data_norm = ((data - np.nanmin(data))/(np.nanmax(data) - np.nanmin(data))) * 225 # min-max normalization to [0, 225] \n",
    "    data_HE = sk.filters.rank.equalize(data_norm.astype(int), footprint=sk.morphology.disk(250))\n",
    "    data_HE = data_HE[100:300, 100:300]\n",
    "    # Assign values - can't just label all of one group as same thing, must look at each segment\n",
    "    bp_min_pix = (bp_max_size / resolution)**2 # 87\n",
    "    bp_min_flux = np.nanmean(data) + 0.25 * np.nanstd(data) \n",
    "    ig_max_flux = np.nanmean(data) - 0.25 * np.nanstd(data)\n",
    "    labeled_preds = skimage.measure.label(preds + 1, connectivity=2)\n",
    "    values = np.unique(labeled_preds) \n",
    "    for value in values:\n",
    "        datavals = data[labeled_preds == value].flatten()\n",
    "        if (np.nanmean(datavals) <= ig_max_flux):\n",
    "            seg[labeled_preds == value] = 0\n",
    "        if (np.nanmean(datavals) > ig_max_flux): #and (len(datavals) > bp_min_pix): #(np.nanmean(datavals) < bp_min_flux):\n",
    "            seg[labeled_preds == value] = 1\n",
    "            if (len(datavals) < bp_min_pix) and np.max(datavals) > bp_min_flux:\n",
    "                seg[labeled_preds == value] = 1.5\n",
    "    return seg\n",
    "\n",
    "seg = kmeans_to_seg(preds, data)\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 3.5))\n",
    "fig.suptitle('KMeans (n_clusters=7, init=\"random\")')\n",
    "ax1.imshow(data, cmap='gray', origin='lower'); ax1.set_title('data')\n",
    "ax2.imshow(preds, origin='lower'); ax2.set_title('kmeans preds')\n",
    "ax3.imshow(seg, origin='lower'); ax3.set_title('kmeans pred -> seg')\n",
    "fig, axs = plt.subplots(n_clusters, 1, figsize=(7, 2*n_clusters))\n",
    "i = 0\n",
    "axs[-1].set_ylabel('Flux')\n",
    "for group in np.unique(preds):\n",
    "    axs[i].hist(data[preds == group], bins=20); axs[i].set_ylabel(f'Group {i}')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2) AgglomorativeCluster ################################################################\n",
    "# #\n",
    "# #       Build nested clusters by merging and splitting them successively\n",
    "# #       Does best with 3 clusters, 'complete' linkage, 'euclidean' metric:\n",
    "# #           Without gradient feature, 3rd cluster becomes rings around granules, not brightpoints\n",
    "# #           With gradient feature, maybe finds dim middles?\n",
    "\n",
    "# from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# n_clusterss = [3] #[2, 3]\n",
    "# metrics = ['euclidean'] #  'l1', 'manhattan' seem to do similarly (at least with n_clusters=2) \n",
    "# linkages = ['complete', 'ward', 'single', 'average'] # 'average' and 'single' do terrible\n",
    "\n",
    "# dict = {}\n",
    "# i = 0\n",
    "# for n_clusters in n_clusterss:\n",
    "#     for metric in metrics:\n",
    "#         for linkage in linkages:\n",
    "#             preds_flat = AgglomerativeClustering(n_clusters=n_clusters, affinity=metric, linkage=linkage).fit(df.values).labels_\n",
    "#             preds = np.reshape(preds_flat, (np.shape(data)[0], np.shape(data)[1]))\n",
    "#             dict[str(i)] = [n_clusters, metric, linkage, preds]\n",
    "#             i += 1\n",
    "\n",
    "# fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(7, 7)) # fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(15, 10))\n",
    "# fig.suptitle('AgglomerativeClustering', color=tc)\n",
    "# axs = [ax1, ax2, ax3, ax4] # [ax1, ax2, ax3, ax4, ax5, ax6]\n",
    "# for i in range(len(axs)):\n",
    "#     n_clusters =  dict[str(i)][0]\n",
    "#     metric = dict[str(i)][1]\n",
    "#     linkage = dict[str(i)][2]\n",
    "#     preds = dict[str(i)][3]\n",
    "#     axs[i].imshow(preds, origin='lower')\n",
    "#     axs[i].set_title('n_clusts='+str(n_clusters)+', metric='+str(metric)+', linkage='+str(linkage), color=tc)\n",
    "\n",
    "# for ax in axs:\n",
    "#     ax.tick_params(labelcolor=tc, color=tc)\n",
    "\n",
    "\n",
    "\n",
    "# # 3) DBSCAN ############################################################################\n",
    "# #\n",
    "# #       Unsupervised algorithm\n",
    "# #       Clusters by identifying areas of high density separated by areas of low density \n",
    "# #       MUCH faster than OPTICS, but same very poor results -  tons of tiny little pixel groups\n",
    "\n",
    "# from sklearn.cluster import DBSCAN\n",
    "\n",
    "# epss = [10, 30] # must be larger for more features (0.5 works fine for 1 feature)\n",
    "# min_sampless = [50, 100, 200]\n",
    "# metric = 'euclidean'\n",
    "# algorithm = 'auto'\n",
    "\n",
    "# dict = {}\n",
    "# i = 0\n",
    "# for eps in epss:\n",
    "#     for min_samples in min_sampless:\n",
    "#         preds_flat = DBSCAN(eps=eps, min_samples=min_samples, metric=metric, algorithm=algorithm, n_jobs=3).fit(df.values).labels_\n",
    "#         preds = np.reshape(preds_flat, (np.shape(data)[0], np.shape(data)[1]))\n",
    "#         dict[str(i)] = [eps, min_samples, preds]\n",
    "#         i += 1\n",
    "\n",
    "# fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(15, 10))\n",
    "# fig.suptitle('DBSCAN', color='white')\n",
    "# axs = [ax1, ax2, ax3, ax4, ax5, ax6]\n",
    "# for i in range(len(axs)):\n",
    "#     eps =  dict[str(i)][0]\n",
    "#     min_samples = dict[str(i)][1]\n",
    "#     preds = dict[str(i)][2]\n",
    "#     axs[i].imshow(preds, origin='lower')\n",
    "#     axs[i].set_title('eps='+str(eps)+', min_samps='+str(min_samples), color=tc)\n",
    "\n",
    "# for ax in axs:\n",
    "#     ax.tick_params(labelcolor=tc, color=tc)\n",
    "\n",
    "\n",
    "# # 3) OPTICS ############################################################################\n",
    "# #\n",
    "# #       Unsupervised algorithm\n",
    "# #       Works simlarly to DBSCAN\n",
    "# #       Performs similarly to DBSCAN but takes longer\n",
    "\n",
    "# from sklearn.cluster import OPTICS\n",
    "\n",
    "# max_epss = [20] # must be larger for more features (0.5 works fine for 1 feature)\n",
    "# min_sampless = [4, 10] # doesn't seem to have much effect \n",
    "# metric = 'euclidean'\n",
    "# algorithm = 'auto'\n",
    "# min_cluster_sizes = [20, 1000, 3000] # [50, 100, 200]\n",
    "# dict = {}\n",
    "# i = 0\n",
    "# for eps in max_epss:\n",
    "#     for min_samples in min_sampless:\n",
    "#         for min_cluster_size in min_cluster_sizes:\n",
    "#             preds_flat = OPTICS(max_eps=eps, min_samples=min_samples, min_cluster_size=min_cluster_size, metric=metric, algorithm=algorithm).fit(df.values).labels_\n",
    "#             preds = np.reshape(preds_flat, (np.shape(data)[0], np.shape(data)[1]))\n",
    "#             dict[str(i)] = [eps, min_samples, min_cluster_size, preds]\n",
    "#             i += 1\n",
    "\n",
    "# fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(15, 10))\n",
    "# fig.suptitle('OPTICS', color=tc)\n",
    "# axs = [ax1, ax2, ax3, ax4, ax5, ax6]\n",
    "# for i in range(len(axs)):\n",
    "#     max_eps =  dict[str(i)][0]\n",
    "#     min_samples = dict[str(i)][1]\n",
    "#     min_cluster_size = dict[str(i)][2]\n",
    "#     preds = dict[str(i)][3]\n",
    "#     axs[i].imshow(preds, origin='lower')\n",
    "#     axs[i].set_title('max_eps='+str(max_eps)+', min_samp='+str(min_cluster_size)+', min_clustsize='+str(min_cluster_size), color='white')\n",
    "\n",
    "# for ax in axs:\n",
    "#     ax.tick_params(labelcolor=tc, color=tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
