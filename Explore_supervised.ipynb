{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.io.fits as fits\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import funclib\n",
    "import skimage\n",
    "import skimage as sk\n",
    "\n",
    "import funclib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ok, so not wieghting (just using MSE) does even worse than using crossentropyloss\n",
    "Things to try: \n",
    "    - Using a weight of 3 for both doesnt help much at all\n",
    "    - Using a weight of 10 for both just reduces GR and IG accuracy, doesnt help BP and DM accuracy (especially DM, BP does do a bit better) so total acc pretty bad\n",
    "    - Even more so for 20\n",
    "    - Raising them both higher and and also making dm more than bp (dm_weight=60, bp_weight=40) -> WTF.Doing that makes the accuracy and bp and dm like zero... \n",
    "Lets try adding input layer that brings out bp, dm??\n",
    "    - Seems to be sort of helpfull? Gets like 11% acc on bps.Wait weird - training with sqaured again goes back to similar acc as with on layer, e.g. like near zero bp, dm acc\n",
    "        Ok current UNet5 is with squared only, but not super usefull   \n",
    "    - Try even more contrast by adding sqaured and fifth power? ---> NO! GIVES TERRIBLE TOTAL ACCURACY AND NEAR ZERO BP, DM ACCURACY\n",
    "    - What about just fith? Still same level of terrible\n",
    "    - Same for 7th (saved as UNet6)\n",
    "    - Weird idea - use diff between binary seg output and inputs as second layer?\n",
    "        Like, train it on a two (or three if perhaps scaling more does get bp) value seg, then took the difference of \n",
    "        that seg and the truths and used that as an input layer to train another model? Wouldnt be using truth values to train or anything. \n",
    "        Would that be sort of similar to \"unsupervised NN\" idea, where it reconstructs inputs using predictions, uses diff between those\n",
    "        and true inputs to train?\n",
    "        Ok, to do this I would need to rerun UNet1 and save the training preds to get the binary NN segs for the training data, cuase I've only saved them\n",
    "        for the val data as of yet. Wait no! Dont want to save the preds it gets on the training data while training, but the preds that the fully trained\n",
    "        model would make on the training preds. Or wait.. but those would probabaly be overfit to. Perhaps really I need to just use the validation outputs from UNet1\n",
    "        as the training data for this UNet (both the imgs and the deltaBinImgs). And then I cant use that as the val data, so maybe use the train data my val data?\n",
    "        Unfortunately, I did split them asymetrically, so there are 953 train files and 397 val files. \n",
    "        Lets not so this - on talking to Kevin it seems like maybe sort of too contrived\n",
    "    - Kevin suggested using histogram normalization (not histogram matching) as a layer to perhaps bring out dm... but on looking at the results of\n",
    "        applying this (as per Kevin's algorithm) seems like it might not be that usefull\n",
    "Lets try training on new data (real data, with segmentations from segment_array_v2)! And also perhaps with some wieghting\n",
    "    - BUT V2 NPY IMAGES ARE LIKE 4X AS BIG AS OTHERS, SO TAKES A HELL OF A LONG TIME \n",
    "    - Could cut them down. Or just cutting down batch size would result in traiing on same num pixels \n",
    "    - BUT probably should cut them down, since cutting batch size would mean most batches are like mostly the same image (less diverse)\n",
    "    - Wait, still takes a very long time..\n",
    "    - Maybe cuase theres just so many images? Lets just train on a subset\n",
    "    - Save as UNet7\n",
    "Huh.. does the similar to before (maybe worse?) where it does good on GR vs IG but terrible on others \n",
    "Tried on just binary data too and it does definitely way worse than trained on first light does\n",
    "    - Save as UNet8\n",
    "Ok, Benoit suggests training to just do IG, GR, BP\n",
    "That does terrible. I wonder if its my custom loss function?\n",
    "Lets try going going back to normal cross entropy loss\n",
    "    - Save as UNet9\n",
    "That works great on the GR, IG. Maybe it is my loss function? Can I weight with the built in? Yes I can!\n",
    "    - try that with bp_wieght = 10\n",
    "    - Save as UNet10\n",
    "Wow that works great! so it is the loss function. Perhaps even getting too many. lets try agian with all 4 classes and a bit lower weight?\n",
    "    - save as UNet11 (use bp_weight = dm_weight = 7)\n",
    "Wow that works great too! Maybe a little too much weighting on both dm and bp (more so on dm). lets try 4 for dm, 5 for bp?\n",
    "    - save as UNet11\n",
    "Now lets see if we can do the same thing but train on real data? \n",
    "    - wait that probably wont work though since didnt work well on binary \n",
    "    - save as UNet12\n",
    "    - Yeah I mean it does ok given the crappy true labels\n",
    "And training on simulation (MURaM) data?\n",
    "    - save as UNet13\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute total validation accuracy\n",
    "pix_correct, ig_correct, dm_correct, gr_correct, bp_correct = 0, 0, 0, 0, 0\n",
    "tot_pix, tot_ig, tot_dm, tot_gr, tot_bp = 0, 0, 0, 0, 0\n",
    "for idx in range(396):\n",
    "    true = np.load(f'../UNet2_outputs/true_{idx}.npy')\n",
    "    preds = np.squeeze(np.load(f'../UNet2_outputs/pred_{idx}.npy'))\n",
    "    pix_correct += len(np.where(preds.flatten() == true.flatten())[0]) #(preds == true).sum()\n",
    "    tot_pix += len(preds.flatten()) # torch.numel(preds)\n",
    "    ig_correct += len(np.where((preds.flatten() == true.flatten()) & (true.flatten() == 0))[0])\n",
    "    tot_ig += len(np.where(true.flatten() == 0)[0])\n",
    "    dm_correct += len(np.where((preds.flatten() == true.flatten()) & (true.flatten() == 0.5))[0])\n",
    "    tot_dm += len(np.where(true.flatten() == 0.5)[0])\n",
    "    gr_correct += len(np.where((preds.flatten() == true.flatten()) & (true.flatten() == 1))[0])\n",
    "    tot_gr += len(np.where(true.flatten() == 1)[0])\n",
    "    bp_correct += len(np.where((preds.flatten() == true.flatten()) & (true.flatten() == 1.5))[0])\n",
    "    tot_bp += len(np.where(true.flatten() == 1.5)[0])\n",
    "\n",
    "print(f\"Total validation accuracy: {pix_correct/tot_pix*100:.2f}%\") \n",
    "print(f'\\t{ig_correct/tot_ig*100:.2f}% of intergranule pixels correctly identified')\n",
    "print(f'\\t{dm_correct/tot_dm*100:.2f}% of dim middle pixels correctly identified')\n",
    "print(f'\\t{gr_correct/tot_gr*100:.2f}% of granule pixels correctly identified')\n",
    "print(f'\\t{bp_correct/tot_bp*100:.2f}% of bright point pixels correctly identified')\n",
    "\n",
    "# Display a random sample of images\n",
    "import cv2\n",
    "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8), (ax9, ax10)) = plt.subplots(5, 2, figsize=(5, 15)); axs = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9, ax10]\n",
    "ax1.set_title('\"True\" Labels')\n",
    "ax2.set_title('Predicted Labels')\n",
    "for i in range(0, 9, 2):\n",
    "    idx = np.random.randint(0, 396)\n",
    "    im = np.load(f'../UNet2_outputs/x_{idx}.npy')[0] # first index to get image\n",
    "    true = np.load(f'../UNet2_outputs/true_{idx}.npy')\n",
    "    preds = np.squeeze(np.load(f'../UNet2_outputs/pred_{idx}.npy'))\n",
    "    axs[i].imshow(true)\n",
    "    axs[i+1].imshow(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNet_name = 'UNet13'\n",
    "n_classes = 4\n",
    "\n",
    "# Compute total validation accuracy\n",
    "output_dir = f'../{UNet_name}_outputs'\n",
    "pct_correct, pct_ig_correct, pct_dm_correct, pct_gr_correct, pct_bp_correct = funclib.compute_validation_results(output_dir, n_classes)\n",
    "print(f'UNet: {UNet_name}')\n",
    "print(f\"Total validation accuracy: {pct_correct:.2f}%\") \n",
    "print(f'\\t{pct_ig_correct}% of intergranule pixels correctly identified')\n",
    "print(f'\\t{pct_dm_correct}% of dim middle pixels correctly identified')\n",
    "print(f'\\t{pct_gr_correct}% of granule pixels correctly identified')\n",
    "print(f'\\t{pct_bp_correct}% of bright point pixels correctly identified')\n",
    "\n",
    "# Display a random sample of images\n",
    "import cv2\n",
    "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8), (ax9, ax10)) = plt.subplots(5, 2, figsize=(5, 15)); axs = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9, ax10]\n",
    "ax1.set_title('\"True\" Labels')\n",
    "ax2.set_title('Predicted Labels')\n",
    "for i in range(0, 9, 2):\n",
    "    idx = np.random.randint(0, 396)\n",
    "    im = np.load(f'{output_dir}/x_{idx}.npy')[0] # first index to get image\n",
    "    true = np.load(f'{output_dir}/true_{idx}.npy')\n",
    "    preds = np.squeeze(np.load(f'{output_dir}/pred_{idx}.npy'))\n",
    "    axs[i].imshow(true, vmin=0, vmax=1.5)\n",
    "    axs[i+1].imshow(preds, vmin=0, vmax=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traditional ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note 1: currently using mostly default tunning parameters - could do more exploration to pick best ones\n",
    "\n",
    "Note 2: currently using validation set from same image as training and test sets.\n",
    "        Ideally could use new DKIST (DKIST_gband_series) for validation. \n",
    "\n",
    "Note 3: Why are validation accuracies higher than test accuracies? And why are test accuracies so high?\n",
    "        Have I screwed something up?\n",
    "        \n",
    "        - Probably is just legit learning the thresholding perfectly, its a very simple metric\n",
    "        - If thats the case, should get ~100% if test against segmented without BPs and DMs (when trained \n",
    "          on no BPs and DMs) and very close when trained on full. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Try out various combinations of training and testing sets\n",
    "'''\n",
    "\n",
    "# Get data (DKIST image) and labels (using our non-ML id method)\n",
    "data_OG = fits.open(\"Data/DKIST_example.fits\")[0].data.byteswap().newbyteorder() # Not sure why this is big-endian to start with..\n",
    "labels_OG = np.squeeze(fits.open(\"Data/DKIST_solarseg_output.fits\")[0].data) # \"c:\\\\Users\\\\lea\\\\OneDrive\\\\DKISTSegmentation\\\\DKISTSegmentation\\\\example_outputs\\\\DKIST\\\\segmented_data_DKIST_example.fits\"\n",
    "labels_noBPDMs = np.squeeze(fits.open(\"Data/DKIST_solarseg_output_noBPDMs.fits\" )[0].data) # all files I created with run_segment_algorithm have first hdu shape (1,x,x)\n",
    "data_50_36 = fits.open('Data/DKIST_gband_series_183653/SEG_VBI_2022_06_03T18_50_36_353_00430500_I_BPDWB_L1.fits')[1].data.byteswap().newbyteorder() # Not sure why this is big-endian to start with..\n",
    "labels_50_36 = np.squeeze(fits.open('Data/DKIST_gband_series_183653/SEG_VBI_2022_06_03T18_50_36_353_00430500_I_BPDWB_L1.fits')[0].data)\n",
    "data_51_58 = fits.open('Data/DKIST_gband_series_183653/SEG_VBI_2022_06_03T18_51_58_613_00430500_I_BPDWB_L1.fits')[1].data.byteswap().newbyteorder() # Not sure why this is big-endian to start with..\n",
    "labels_51_58 = np.squeeze(fits.open('Data/DKIST_gband_series_183653/SEG_VBI_2022_06_03T18_51_58_613_00430500_I_BPDWB_L1.fits')[0].data)\n",
    "\n",
    "# Prepare half (really 1/4) data for training and testing (use second half for first validation)\n",
    "tt_data_OG = data_OG[0:int(np.shape(data_OG)[0]/2), 0:int(np.shape(data_OG)[1]/2)]\n",
    "tt_labels_OG = labels_OG[0:int(np.shape(labels_OG)[0]/2), 0:int(np.shape(labels_OG)[1]/2)]\n",
    "X_tt_OG, Y_tt_OG = funclib.pre_proccess(tt_data_OG, tt_labels_OG)\n",
    "X_train_OG, X_test_OG, Y_train_OG, Y_test_OG = train_test_split(X_tt_OG, Y_tt_OG, test_size=0.4, random_state=20)\n",
    "\n",
    "# Create validation set Y and Y using same steps as above\n",
    "val_data_OG = data_OG[int(np.shape(data_OG)[0]/2):-1, int(np.shape(data_OG)[1]/2):-1]\n",
    "val_labels_OG = labels_OG[int(np.shape(labels_OG)[0]/2):-1, int(np.shape(labels_OG)[1]/2):-1] #labels[int(np.shape(labels)[0]/2):-1, int(np.shape(labels)[1]/2):-1]\n",
    "X_val_OG, Y_val_OG = funclib.pre_proccess(val_data_OG, val_labels_OG)\n",
    "\n",
    "# Create \"secondary\" train and test sets (with a new data file)\n",
    "X_tt_51_58, Y_tt_51_58 = funclib.pre_proccess(data_51_58, labels_51_58)\n",
    "X_train_51_58, X_test_51_58, Y_train_51_58, Y_test_51_58 = train_test_split(X_tt_51_58, Y_tt_51_58, test_size=0.4, random_state=20)\n",
    "\n",
    "# Create \"secondary\" validation set (a different new data file)\n",
    "X_val_50_36, Y_val_50_36 = funclib.pre_proccess(data_50_36, labels_50_36)\n",
    "\n",
    "# Create combined training set - WILL THE BOUNDARY CREATE PROBLEMS??? BUT I GUESS TRAIN_TEST_SPLIT (WHICH HONESTLY I SHOULDNT DO) WILL REMOVE THAT ISSUE\n",
    "dataset1 = data_OG[0:int(np.shape(data_OG)[0]/2), 0:int(np.shape(data_OG)[1]/4)]\n",
    "dataset2 = data_51_58[100:np.shape(dataset1)[0]+100, 100:np.shape(dataset1)[1]+100] # shift by 100 to aviod constant boundary\n",
    "data_comb = np.concatenate((dataset1, dataset2), axis=1)\n",
    "labelset1 = labels_OG[0:int(np.shape(labels_OG)[0]/2), 0:int(np.shape(labels_OG)[1]/4)]\n",
    "labelset2 = labels_51_58[100:np.shape(labelset1)[0]+100, 100:np.shape(labelset1)[1]+100] # shift by 100 to aviod constant boundary\n",
    "labels_comb = np.concatenate((labelset1, labelset2), axis=1)\n",
    "X_tt_comb, Y_tt_comb = funclib.pre_proccess(data_comb, labels_comb)\n",
    "X_train_comb, X_test_comb, Y_train_comb, Y_test_comb = train_test_split(X_tt_comb, Y_tt_comb, test_size=0.4, random_state=20)\n",
    "\n",
    "# 1) RandomForestClassifier ###############################################################\n",
    "\n",
    "# Train on OG train set\n",
    "print('RF Trained on OG')\n",
    "model = RandomForestClassifier(n_estimators=10)\n",
    "model.fit(X_train_OG, Y_train_OG)\n",
    "\n",
    "# Predict on OG test set\n",
    "pred_test_OG =  model.predict(X_test_OG)\n",
    "print('     accuracy on OG test set: ', metrics.accuracy_score(Y_test_OG, pred_test_OG))\n",
    "\n",
    "# Predict on OG validation set\n",
    "pred_val_OG =  model.predict(X_val_OG)\n",
    "print('     accuracy on OG validation set: ', metrics.accuracy_score(Y_val_OG, pred_val_OG))\n",
    "\n",
    "# Predict on validation set 2 \n",
    "pred_val_50_36 =  model.predict(X_val_50_36)\n",
    "print('     accuracy on 50_36 validation set: ', metrics.accuracy_score(Y_val_50_36, pred_val_50_36))\n",
    "\n",
    "# Predict on combined test set\n",
    "pred_test_comb =  model.predict(X_test_comb)\n",
    "print('     accuracy on combined test set: ', metrics.accuracy_score(Y_test_comb, pred_test_comb))\n",
    "\n",
    "# Train on 51_58 set \n",
    "print('RF trained on 51_58')\n",
    "model = RandomForestClassifier(n_estimators=10)\n",
    "model.fit(X_train2, Y_train2)\n",
    "\n",
    "# Predict on OG validation set\n",
    "pred_val_OG =  model.predict(X_val_OG)\n",
    "print('     accuracy on OG validation set: ', metrics.accuracy_score(Y_val_OG, pred_val_OG))\n",
    "\n",
    "# Predict on validation set 2 \n",
    "pred_val_50_36 =  model.predict(X_val_50_36)\n",
    "print('     accuracy on 50_36 validation set: ', metrics.accuracy_score(Y_val_50_36, pred_val_50_36))\n",
    "\n",
    "# Predict on combined test set\n",
    "pred_test_comb =  model.predict(X_test_comb)\n",
    "print('     accuracy on combined test set: ', metrics.accuracy_score(Y_test_comb, pred_test_comb))\n",
    "\n",
    "# Train on combined set \n",
    "print('RF trained on combined')\n",
    "model = RandomForestClassifier(n_estimators=10)\n",
    "model.fit(X_train_comb, Y_train_comb)\n",
    "\n",
    "# Predict on OG validation set\n",
    "pred_val_OG =  model.predict(X_val_OG)\n",
    "print('     accuracy on OG validation set: ', metrics.accuracy_score(Y_val_OG, pred_val_OG))\n",
    "\n",
    "# Predict on validation set 2 \n",
    "pred_val_50_36 =  model.predict(X_val_50_36)\n",
    "print('     accuracy on 50_36 validation set: ', metrics.accuracy_score(Y_val_50_36, pred_val_50_36))\n",
    "\n",
    "# Predict on combined test set\n",
    "pred_test_comb =  model.predict(X_test_comb)\n",
    "print('     accuracy on combined test set: ', metrics.accuracy_score(Y_test_comb, pred_test_comb))\n",
    "\n",
    "# Plot\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10, 3))\n",
    "plt.suptitle('Random Forest (trained and tested on OG)', color=tc)\n",
    "axs[0].imshow(val_data)\n",
    "axs[0].set_title('Validation Data', color=tc)\n",
    "axs[1].imshow(Y_val.reshape(np.shape(val_data)[0], np.shape(val_data)[1]))\n",
    "axs[1].set_title('Validation True Labels', color=tc)\n",
    "axs[2].imshow(pred_val.reshape(np.shape(val_data)[0], np.shape(val_data)[1]))\n",
    "axs[2].set_title('Validation Predicted Labels', color=tc)\n",
    "for ax in axs:\n",
    "    ax.tick_params(labelcolor=tc, color=tc)\n",
    "\n",
    "# 2) KNeighborsClassifier ###############################################################\n",
    "\n",
    "# Train on OG train set\n",
    "print('KNN Trained on OG')\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(X_train_OG, Y_train_OG)\n",
    "\n",
    "# Predict on OG test set\n",
    "pred_test_OG =  model.predict(X_test_OG)\n",
    "print('     accuracy on OG test set: ', metrics.accuracy_score(Y_test_OG, pred_test_OG))\n",
    "\n",
    "# Predict on OG validation set\n",
    "pred_val_OG =  model.predict(X_val_OG)\n",
    "print('     accuracy on OG validation set: ', metrics.accuracy_score(Y_val_OG, pred_val_OG))\n",
    "\n",
    "# Predict on validation set 2 \n",
    "pred_val_50_36 =  model.predict(X_val_50_36)\n",
    "print('     accuracy on 50_36 validation set: ', metrics.accuracy_score(Y_val_50_36, pred_val_50_36))\n",
    "\n",
    "# Predict on combined test set\n",
    "pred_test_comb =  model.predict(X_test_comb)\n",
    "print('     accuracy on combined test set: ', metrics.accuracy_score(Y_test_comb, pred_test_comb))\n",
    "\n",
    "# Train on 51_58 set \n",
    "print('KNN trained on 51_58')\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(X_train_51_58, Y_train_51_58)\n",
    "\n",
    "# Predict on OG validation set\n",
    "pred_val_OG =  model.predict(X_val_OG)\n",
    "print('     accuracy on OG validation set: ', metrics.accuracy_score(Y_val_OG, pred_val_OG))\n",
    "\n",
    "# Predict on validation set 2 \n",
    "pred_val_50_36 =  model.predict(X_val_50_36)\n",
    "print('     accuracy on 50_36 validation set: ', metrics.accuracy_score(Y_val_50_36, pred_val_50_36))\n",
    "\n",
    "# Predict on combined test set\n",
    "pred_test_comb =  model.predict(X_test_comb)\n",
    "print('     accuracy on combined test set: ', metrics.accuracy_score(Y_test_comb, pred_test_comb))\n",
    "\n",
    "# Train on combined set \n",
    "print('KNN trained on combined')\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(X_train_comb, Y_train_comb)\n",
    "\n",
    "# Predict on OG validation set\n",
    "pred_val_OG =  model.predict(X_val_OG)\n",
    "print('     accuracy on OG validation set: ', metrics.accuracy_score(Y_val_OG, pred_val_OG))\n",
    "\n",
    "# Predict on validation set 2 \n",
    "pred_val_50_36 =  model.predict(X_val_50_36)\n",
    "print('     accuracy on 50_36 validation set: ', metrics.accuracy_score(Y_val_50_36, pred_val_50_36))\n",
    "\n",
    "# Predict on combined test set\n",
    "pred_test_comb =  model.predict(X_test_comb)\n",
    "print('     accuracy on combined test set: ', metrics.accuracy_score(Y_test_comb, pred_test_comb))\n",
    "\n",
    "# Plot\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10, 3))\n",
    "plt.suptitle('KNN (trained and tested on OG)', color=tc)\n",
    "axs[0].imshow(val_data)\n",
    "axs[0].set_title('Validation Data', color=tc)\n",
    "axs[1].imshow(Y_val.reshape(np.shape(val_data)[0], np.shape(val_data)[1]))\n",
    "axs[1].set_title('Validation True Labels', color=tc)\n",
    "axs[2].imshow(pred_val.reshape(np.shape(val_data)[0], np.shape(val_data)[1]))\n",
    "axs[2].set_title('Validation Predicted Labels', color=tc)\n",
    "for ax in axs:\n",
    "    ax.tick_params(labelcolor=tc, color=tc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Same as above but simplify - create one big combined set to take train/test/val data from\n",
    "'''\n",
    "\n",
    "# Get data (DKIST image) and labels \n",
    "data = fits.open(\"Data/DKIST_example.fits\")[0].data.byteswap().newbyteorder()[0:1365, 0:1365]\n",
    "labels = np.squeeze(fits.open(\"Data/DKIST_solarseg_output.fits\")[0].data)[0:1365, 0:1365]\n",
    "for file in os.listdir('Data/DKIST_gband_series_183653/'):\n",
    "        if file.startswith('SEG_'):\n",
    "            path = 'Data/DKIST_gband_series_183653/'+file\n",
    "            data1 = fits.open(path)[1].data.byteswap().newbyteorder()\n",
    "            data = np.concatenate((data, data1), axis=1)\n",
    "            labels1 = np.squeeze(fits.open(path)[0].data)\n",
    "            labels = np.concatenate((labels, labels1), axis=1)\n",
    "\n",
    "# Prepare half for training and testing (split along 'short' axis to encompass all)\n",
    "tt_data = data[0:int(np.shape(data)[0]/2), :]\n",
    "tt_labels = labels[0:int(np.shape(data)[0]/2), :]\n",
    "X_tt, Y_tt = funclib.pre_proccess(tt_data, tt_labels)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_tt, Y_tt, test_size=0.4, random_state=20)\n",
    "\n",
    "# Prepare other half for validation\n",
    "tt_data = data[int(np.shape(data)[0]/2):-1, :]\n",
    "tt_labels = labels[int(np.shape(data)[0]/2):-1, :]\n",
    "X_val, Y_val = funclib.pre_proccess(tt_data, tt_labels)\n",
    "\n",
    "\n",
    "# 1) RandomForestClassifier ###############################################################\n",
    "\n",
    "# Train on train set\n",
    "print('RF')\n",
    "model = RandomForestClassifier(n_estimators=10)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on test set\n",
    "pred_test =  model.predict(X_test)\n",
    "print('     accuracy on test set: ', metrics.accuracy_score(Y_test, pred_test))\n",
    "\n",
    "# Predict on OG validation set\n",
    "pred_val =  model.predict(X_val)\n",
    "print('     accuracy on validation set: ', metrics.accuracy_score(Y_val, pred_val))\n",
    "\n",
    "# # 2) KNeighborsClassifier ###############################################################\n",
    "\n",
    "# # Train on train set\n",
    "# print('KNN')\n",
    "# model = KNeighborsClassifier(n_neighbors=5)\n",
    "# model.fit(X_train, Y_train)\n",
    "\n",
    "# # Predict on test set\n",
    "# pred_test =  model.predict(X_test)\n",
    "# print('     accuracy on test set: ', metrics.accuracy_score(Y_test, pred_test))\n",
    "\n",
    "# # Predict on OG validation set\n",
    "# pred_val =  model.predict(X_val)\n",
    "# print('     accuracy on validation set: ', metrics.accuracy_score(Y_val, pred_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add values of different Gabor filters as features\n",
    "# num = 1\n",
    "# kernels = []\n",
    "# for theta in range(2):\n",
    "#     theta = theta/4 * np.pi\n",
    "#     for sigma in [1,3]:\n",
    "#         for lambd in np.arange(0, np.pi, np.pi/4):\n",
    "#             for gamma in [0.05, 0,5]:\n",
    "#                 kernel = cv2.getGaborKernel((5, 5), sigma, theta, lambd, gamma, 0, ktype=cv2.CV_32F)\n",
    "#                 kernels.append(kernel)\n",
    "#                 filtered_img = cv2.filter2D(data1, cv2.CV_8UC3, kernel).reshape(-1)\n",
    "#                 df['Gabor'+str(num)] = filtered_img\n",
    "#                 num =  num + 1\n",
    "#\n",
    "# Add Canny edge detection as another feature\n",
    "# edges = cv2.Canny(data).reshape(-1)\n",
    "# df['Canny_edges'] = edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
